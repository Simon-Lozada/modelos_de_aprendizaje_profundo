{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ceb718a",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ab19ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 20:20:30.670627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oracle/instantclient_19_8\n",
      "2021-12-08 20:20:30.670671: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 20:20:33.608916: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/oracle/instantclient_19_8\n",
      "2021-12-08 20:20:33.608960: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-08 20:20:33.608981: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (virtual-machine): /proc/driver/nvidia/version does not exist\n",
      "2021-12-08 20:20:33.609329: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "\n",
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir)\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )\n",
    "\n",
    "train_data = prepareData('data/aclImdb/test')\n",
    "test_data = prepareData('data/aclImdb/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fb77b",
   "metadata": {},
   "source": [
    "Ahora, todas las instancias `<br />` en nuestro conjunto de datos han sido reemplazadas por espacios.  Puede intentar imprimir parte del conjunto de datos si lo desea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f437703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Another popular screening for a British picture at Coalville's Century Theatre. A well crafted, solid drama with an ever developing plot and ongoing 'twists in the tale'...as the lies piled up! A masterclass of acting by a flawless cast, well marshaled by first time director Julian Fellowes. Outstanding performance, as usual, by Tom Wilkinson but good turns by all concerned including supporting actors Linda Bassett and John Neville. Our audience was engrossed by this film, which includes a couple of shock incidents which really make you 'jump'. A good tight production at around only 80 minutes, probably produced on a very limited budget, but a success, which should see Fellowes directing again for the big screen. Some publicity for the film seemed to suggest it was set in the 50s (as per Nigel Balchin's novel)but obviously this is not the case. Recommended viewing.\"\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(1):\n",
    "  print(text_batch.numpy()[0])\n",
    "  print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d83913",
   "metadata": {},
   "source": [
    "## Construyendo el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d9685",
   "metadata": {},
   "source": [
    "Usaremos la Sequential clase , que representa una pila lineal de capas. Para empezar, crearemos una instancia de un modelo secuencial vacío y definiremos su tipo de entrada: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e118757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape = (1,), dtype = \"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a5327",
   "metadata": {},
   "source": [
    "Nuestro modelo ahora toma 1 entrada de cadena: tiempo para hacer algo con esa cadena. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4da6e",
   "metadata": {},
   "source": [
    "## 3.1 Vectorización de texto "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23f980",
   "metadata": {},
   "source": [
    "Nuestra primera capa será una capa TextVectorization, que procesará la cadena de entrada y la convertirá en una secuencia de números enteros, cada uno representando un token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5284cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  \n",
    "  # Tamaño máximo de vocabulario. Cualquier palabra fuera de las max_tokens más comunes\n",
    "  # serán tratados de la misma manera: como tokens \"fuera de vocabulario\" (OOV).\n",
    "  max_tokens = max_tokens,\n",
    "  \n",
    "  # Salida de índices enteros, uno por token de cadena\n",
    "  output_mode = \"int\",\n",
    "  \n",
    "  # Rellene o trunque siempre exactamente esta cantidad de tokens\n",
    "  output_sequence_length = max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ec200",
   "metadata": {},
   "source": [
    "Para inicializar la capa, necesitamos llamar a .adapt ():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a7d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llame a adapt (), que ajusta la capa TextVectorization a nuestro dataset de texto.\n",
    "# Aquí es cuando se seleccionan las palabras más comunes de max_tokens (es decir, el vocabulario).\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba1068",
   "metadata": {},
   "source": [
    "## 3.2 Incrustación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53263f1a",
   "metadata": {},
   "source": [
    "Nuestra siguiente capa será una incrustación, que convertirá los números enteros producidos por la capa anterior en vectores de longitud fija."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbafde0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Capa anterior: Vectorización de texto\n",
    "max_tokens = 1000\n",
    "\n",
    "model.add(vectorize_layer)\n",
    "\n",
    "# Tenga en cuenta que estamos usando max_tokens + 1 aquí, ya que hay un token fuera de vocabulario (OOV) que se agrega al vocabulario.\n",
    "model.add(Embedding(max_tokens + 1, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06725a34",
   "metadata": {},
   "source": [
    "## 3.3 La capa recurrente "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bde04f",
   "metadata": {},
   "source": [
    "¡Finalmente, estamos listos para la capa recurrente que convierte a nuestra red en una RNN!  Usaremos una capa Long Short-Term Memory (LSTM), que es una opción popular para este tipo de problema.  Es muy sencillo de implementar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33527102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# 64 es el parámetro \"unidades\", que es la dimensionalidad del espacio de salida.\n",
    "model.add(LSTM(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364c2ba",
   "metadata": {},
   "source": [
    "Para terminar nuestra red, agregaremos una estándar completamente conectada ( Densa capa ) y una capa de salida con sigmoidea activación : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e28094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2435992",
   "metadata": {},
   "source": [
    "## 4. Compilación del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fdd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer = 'adam',\n",
    "  loss = 'binary_crossentropy',\n",
    "  metrics = ['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e3d78",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e104237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 46s 57ms/step - loss: 0.5456 - accuracy: 0.7221\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.4297 - accuracy: 0.8043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b294ce370>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c5f4e",
   "metadata": {},
   "source": [
    "Usar el modelo entrenado para hacer predicciones es fácil: pasamos una cadena a predict()y genera una partitura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "699ba8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9096129]]\n",
      "[[0.162857]]\n"
     ]
    }
   ],
   "source": [
    "# Debería imprimir una puntuación muy alta como 0,90.\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "# Debería imprimir una puntuación muy baja como 0,01.\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3b715",
   "metadata": {},
   "source": [
    "https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979fc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
